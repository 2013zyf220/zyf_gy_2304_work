View(mtcars)
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP.R")
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
View(mtcars)
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
View(mtcars)
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
source("C:/Users/zyf20/Downloads/第2讲 简单线性回归TEMP2.R")
View(results)
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
source("E:/zyf_gy/zyf_gy_2304_work/工具/R语言相关/R油管教程/第2讲 简单线性回归.R")
hist_1b_v <- matrix(-9999, nrow = year_len, ncol = 8);
year_s <- 2018 #to_be_set
year_e <- 2019 #to_be_set
year_len <- year_e - year_s + 1
hist_1b_v <- matrix(-9999, nrow = year_len, ncol = 8);
View(hist_1b_v)
#=================================================================
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
#setwd('E:/zyf_gn/zyf_gn_2301_data/ppa_2301_k2');
p_load(randomForest, caret, pROC)
data('iris')
summary(iris)
#dim(iris)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE)
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#=================================================================
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
rf.train
source("~/.active-rstudio-document")
installed.packages(skimr)
installed.packages('skimr')
library(skimr)
installed.packages('skimr')
installed.packages('skimr')
library(DataExplorer)
installed.packages('DataExplorer')
installed.packages('caret')
installed.packages('caret')
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
#input data
data('iris')
summary(iris)
dim(iris)
View(iris)
View(iris)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
View(trainlist)
View(trainlist)
View(testset)
dim(trainset)
dim(testset)
#=================================================================
set.seed(1)
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#dim(iris)
View(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
#=================================================================
set.seed(1)
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
plot(rf.train, main = 'randomforest origin');
set.seed(2)
rf.test <- predict(rf.train, newdata = testset, type = 'class') #该分析预测种类，因此type为class
rf.test
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species)) #as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
data('iris')
summary(iris)
#dim(iris)
library(pacman)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
source("E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/random_forest_1_up2023_1026.R")
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#dim(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
plot(rf.train, main = 'randomforest origin');
set.seed(2)
rf.test <- predict(rf.train, newdata = testset, type = 'class') #该分析预测种类，因此type为class
rf.test
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species))
#as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
library(randomForest)
library(skimr)
library(DataExplorer)
library(caret)
library(pROC)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/data-Boston4Reg.csv')
library(randomForest)
library(skimr)
library(DataExplorer)
library(caret)
library(pROC)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/data-Boston4Reg.csv')
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
skim(boston) #鸟瞰数据
View(boston)
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
na.roughfix(boston) #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
View(boston)
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#dim(iris)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
#set model
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
set.seed(2)
rf.test <- predict(rf.train, newdata = testset, type = 'class') #该分析预测种类，因此type为class
rf.test
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species))
#as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
#ROC曲线和AUC值
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species))
#as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
#结果需查看：Confusion Ma
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越
library(randomForest)
library(skimr)
library(DataExplorer)
library(caret)
library(pROC)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
na.roughfix(boston) #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
skim(boston) #再次鸟瞰数据
hist(boston$medv, breaks = 50) #对因变量做直方图
hist(boston$medv, breaks = 30) #对因变量做直方图
hist(boston$medv, breaks = 50) #对因变量做直方图
hist(boston$medv, breaks = 20) #对因变量做直方图
hist(boston$medv, breaks = 50) #对因变量做直方图
set.seed(1)
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
#分类数据
set.seed(1)
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
form_reg <- as.formula(
paste0(
'medv ~',
paste(colnames(traindata)[1:13], collapse = ' + ')
)
)
form_reg
set.seed(2)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg#模型概要
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg#模型概要
plot(fit_rf_reg, main = "树的棵树与袋外MSE") # ntree参数与error之间的关系图示
library(tidyverse)
hist(boston$medv, breaks = 50) #对因变量做直方图
View(trains)
View(testdata)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
form_reg
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
# 变量重要性
importance(fit_rf_reg)
plot(fit_rf_reg, main = "树的棵树与袋外MSE") # ntree参数与error之间的关系图示
# 变量重要性图示，默认最多显示30个变量
varImpPlot(fit_rf_reg, main = "随机森林变量重要性")
# 偏依赖图: 因变量随特定自变量变化的变化
partialPlot(x = fit_rf_reg, pred.data = traindata, x.var = crim)
#确保pred.data数据为data.frame, x.var: 某个自变量名称
plot(medv ~ crim, data = traindata) #对应散点分布
# 偏依赖图: 因变量随特定自变量变化的变化
partialPlot(x = fit_rf_reg, pred.data = traindata, x.var = crim)
# 预测
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainlinmod <- lm(trainpred ~ traindata$medv)
abline(trainlinmod, col = 'blue', lwd = 2.5, lty = 'solid')
abline(a = 0, b = 1, col = 'red', lwd = 2.5, lty = 'dashed')
legend('topleft', legend = c('Model', 'Base'), col = c('blue', 'red'), lwd = 2.5, lty = c('solid','dashed'))
#测试集预测结果
testpred <- predict(fit_rf_reg, newdata = testdata)
testpred <- predict(fit_rf_reg, newdata = testdata)  #测试集预测结果
defaultSummary(data.frame(obs = testdata$medv, pred = testpred))  #测试集预测误差指标
plot(x = testdata$medv, y = testpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林——实际值与预测值比较", sub = "测试集") # 图示测试集预测结果
testlinmod <- lm(testpred ~testdata$medv)
abline(testlinmod, col = 'blue', lwd = 2.5, lty = 'solid')
abline(a = 0, b = 1, col = 'red', lwd = 2.5, lty = 'dashed')
legend('topleft', legend = c('Model', 'Base'), col = c('blue', 'red'), lwd = 2.5, lty = c('solid','dashed'))
predresult <- data.frame(obs = c(traindata$medv, testdata$medv), pred = c(trainpred, testpred),
group = c(rep('Train', length(trainpred)), rep('Test', length(testpred))))
ggplot(predresult, aes(x = obs, y = pred, fill = group, colour = group)) +
geom_point(shape = 21, size = 3) +
geom_smooth(method = 'lm', se = F, size = 1.2) +
geom_abline(intercept = 0, slope = 1, size = 1.2) +
labs(fill = NULL, colour = NULL) +
theme(legend.position = 'bottom')
predresult <- data.frame(obs = c(traindata$medv, testdata$medv), pred = c(trainpred, testpred),
group = c(rep('Train', length(trainpred)), rep('Test', length(testpred))))
View(predresult)
library(pacman)
p_load(randomForest, caret, pROC)
data(iris) #加载数据
str(iris) #查看数据结构
View(iris)
install.packages("caTools")
library(pacman)
p_load(randomForest, caret, pROC, caTools)
data(iris) #加载数据
str(iris) #查看数据结构
split <- sample.split(iris, SplitRatio = 0.7) #训练数据占原数据的70%
split
split
train <- subset(iris, split == "TRUE")
test <- subset(iris, split == "FALSE")
split <- sample.split(iris, SplitRatio = 0.7) #训练数据占原数据的70%
split
train <- subset(iris, split == "TRUE")
test <- subset(iris, split == "FALSE")
View(test)
split <- sample.split(iris, SplitRatio = 0.7) #训练数据占原数据的70%
split
split <- sample.split(iris, SplitRatio = 0.8) #训练数据占原数据的70%
split
split <- sample.split(iris, SplitRatio = 0.75) #训练数据占原数据的70%
split
library(MASS)
data(Boston)
str(Boston)
set.seed(123)
split<- sample(nrow(Boston), nrow(Boston)*0.7)
train <- Boston[split, ]
test <- Boston[-split, ]
split<- sample(nrow(Boston), nrow(Boston)*0.7)
train <- Boston[split, ]
test <- Boston[-split, ]
library(gbm)
mod1 = gbm(medv ~ ., data = Boston)
mod1
mod2 <- gbm.fit(x=Boston[,1:13],y=Boston[,14],)
mod2
View(mod2)
fit_1 <- gbm(medv~., data = train, verbose = TRUE, shrinkage = 0.01, interaction.depth = 3,
n.minobsinnode = 5, n.trees = 5000, cv.folds = 10)
print(fit_1)
summary(fit_1)
perf_gbm1 = gbm.perf(fit_1, method = "cv")
perf_gbm1
predict1 <- predict(fit_1, test, perf_gbm1) #生成预测数据
plot(test$medv, predict1, main = 'Test dataset', xlab = 'original data', ylab = 'Predicted data')
#将原数据值和预测值作散点图
abline(1, 1)
plot.gbm(fit_1, i.var = 1) #生成第1个变量crim的偏依赖图
plot.gbm(fit_1, i.var = c(1,5)) #生成第1个和第5个变量的偏依赖图
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
data('iris')
summary(iris)
#dim(iris)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
plot(rf.train, main = 'randomforest origin');
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
data('iris')
summary(iris)
#dim(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
library(pacman)
p_load(randomForest, caret, pROC)
#input data
data('iris')
summary(iris)
#dim(iris)
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#dim(trainset)
#dim(testset)
#set model
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
plot(rf.train, main = 'randomforest origin');
set.seed(2)
rf.test <- predict(rf.train, newdata = testset, type = 'class') #该分析预测种类，因此type为class
rf.test
#结果统计
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species))
#as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
library(randomForest)
library(skimr)
library(DataExplorer)
library(tidyverse)
library(caret)
library(pROC)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
#as.data.frame()
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
#na.roughfix() #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
skim(boston) #再次鸟瞰数据
hist(boston$medv, breaks = 50) #对因变量做直方图
#分类数据
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
#构建公式
form_reg <- as.formula(paste0('medv ~', paste(colnames(traindata)[1:13], collapse = ' + '))) #as.formula(): 将字符串转换成公式
form_reg
#modelling
set.seed(2)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
plot(fit_rf_reg, main = "树的棵树与袋外MSE") # ntree参数与error之间的关系图示
# 变量重要性图示，默认最多显示30个变量
importance(fit_rf_reg) #变量重要性
varImpPlot(fit_rf_reg, main = "随机森林变量重要性")
# 偏依赖图: 因变量随特定自变量变化的变化
partialPlot(x = fit_rf_reg, pred.data = traindata, x.var = crim)
#确保pred.data数据为data.frame, x.var: 某个自变量名称
plot(medv ~ crim, data = traindata) #对应散点图
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainlinmod <- lm(trainpred ~ traindata$medv)
abline(trainlinmod, col = 'blue', lwd = 2.5, lty = 'solid')
abline(a = 0, b = 1, col = 'red', lwd = 2.5, lty = 'dashed')
legend('topleft', legend = c('Model', 'Base'), col = c('blue', 'red'), lwd = 2.5, lty = c('solid','dashed'))
testpred <- predict(fit_rf_reg, newdata = testdata)  #测试集预测结果
defaultSummary(data.frame(obs = testdata$medv, pred = testpred))  #测试集预测误差指标
plot(x = testdata$medv, y = testpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林——实际值与预测值比较", sub = "测试集") # 图示测试集预测结果
testlinmod <- lm(testpred ~testdata$medv)
abline(testlinmod, col = 'blue', lwd = 2.5, lty = 'solid')
abline(a = 0, b = 1, col = 'red', lwd = 2.5, lty = 'dashed')
legend('topleft', legend = c('Model', 'Base'), col = c('blue', 'red'), lwd = 2.5, lty = c('solid','dashed'))
predresult <- data.frame(obs = c(traindata$medv, testdata$medv), pred = c(trainpred, testpred),
group = c(rep('Train', length(trainpred)), rep('Test', length(testpred))))
ggplot(predresult, aes(x = obs, y = pred, fill = group, colour = group)) +
geom_point(shape = 21, size = 3) +
geom_smooth(method = 'lm', se = F, size = 1.2) +
geom_abline(intercept = 0, slope = 1, size = 1.2) +
labs(fill = NULL, colour = NULL) +
theme(legend.position = 'bottom')
form_reg
source("E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/BRT_1_up2023_1026.R")
