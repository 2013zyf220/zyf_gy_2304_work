grid_1 = raster('E:/zyf_gy/zyf_gy_temp/LC09_L2SP_128039_20230908_20230913_02_T1_ST_B10.TIF'); #to_be_set
plot(grid_1)
grid_2 <- projectRaster(grid_1, '+init=epsg:4326')
grid_2 <- projectRaster(grid_1, crs = '+init=epsg:4326')
grid_1 = raster('E:/zyf_gy/zyf_gy_temp/LC09_L2SP_128039_20230908_20230913_02_T1_ST_B10_rep.TIF'); #to_be_set
plot(grid_1)
new_extent <- extent(xmin = 106.3, xmax = 107, ymin = 29.3, ymax = 30)
library(raster)
new_extent <- extent(xmin = 106.3, xmax = 107, ymin = 29.3, ymax = 30)
new_extent <- extent(xmin = 106.3, xmax = 107, ymin = 29.3, ymax = 30)
new_extent <- extent(106.3,107, 29.3, 30)
grid_2 <- crop(grid_1, new_extent)
plot(grid_2)
grid_3 <- grid_2 * 0.00341802 +149 - 273.15
plot(grid_2)
plot(grid_3)
plot(raster_data, col = color_palette(100))
color_palette <- colorRampPalette(c("blue", "red"))
plot(raster_data, col = color_palette(100))
plot(grid_3, col = color_palette(100))
source("E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/BRT_2_v1_ing2023_1105.R")
library(MASS)
library(gbm)
library(randomForest)
library(skimr)
library(DataExplorer)
library(tidyverse)
library(caret)
library(pROC)
index_y_sel <- 'rx_rci'; #to_be_set
col_values_f <- function(f_mat, f_col_name){
if (f_col_name %in% colnames(f_mat)) {
col_index <- which(colnames(f_mat) == f_col_name)  # Find the column index
col_values <- f_mat[, col_index]  # Extract the column values
return(col_values)
} else {
cat("Column name not found in the matrix.")
return(NULL)
}
}
data_1 <- read.csv('E:/zyf_gn/zyf_gn_2301_data/ppa_2301_k2/shp/outputs2/2301_river_6_2021.csv'); #to_be_set
data_2 <- data_1[,c(3,7,16,18,20,22,33,34,35,48)] #to_be_set
skim(data_2) #鸟瞰数据
plot_missing(data_2) #数据缺失状况
data_2v <- col_values_f(data_2, index_y_sel)
hist(data_2v, breaks = 50) #to_be_set #对因变量做直方图
#分类数据
set.seed(1)
trains <- createDataPartition(y = data_2v, p = 0.75, list = F); #to_be_set #基于因变量拆分
train_data <- data_2[trains, ]
test_data <- data_2[-trains, ]
train_data_v <- col_values_f(train_data, index_y_sel)
test_data_v <- col_values_f(test_data, index_y_sel)
hist(train_data_v, breaks = 50) #to_be_set
hist(test_data_v, breaks = 50) #to_be_set
colnames(data_2) #get the column names of data_2
#构建公式
col_ext <- c(1, 2, 3, 4, 5, 6, 10) #to_be_set
form_reg <- as.formula(paste0(index_y_sel,' ~', paste(colnames(train_data)[col_ext], collapse = ' + '))) #to_be_set #as.formula(): 将字符串转换成公式
form_reg
set.seed(123)
fit_1 <- gbm(form_reg, data = train_data, verbose = TRUE, shrinkage = 0.01, interaction.depth = 3,
n.minobsinnode = 5, n.trees = 5000, cv.folds = 10)
print(fit_1)
summary(fit_1)
perf_gbm1 = gbm.perf(fit_1, method = "cv")
perf_gbm1
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
# Load the required packages
library(raster)
library(sf)
# Specify the path to your shapefile (replace "your_shapefile.shp" with the actual file name)
shapefile_path <- 'E:/zyf_gy/zyf_gy_temp/temp22.tif'
# Read the shapefile using sf
sf_object <- st_read(shapefile_path)
# Specify the path to your shapefile (replace "your_shapefile.shp" with the actual file name)
shapefile_path <- 'E:/zyf_gy/zyf_gy_temp/bh_1.shp'
# Read the shapefile using sf
sf_object <- st_read(shapefile_path)
# Create an empty raster layer with the desired extent and resolution
# You can customize the extent and resolution based on your data
raster_layer <- raster(extent(sf_object), res = c(0.1, 0.1))
# Use rasterize to transfer values from sf_object$a1 to the raster layer
rasterized_layer <- rasterize(sf_object, raster_layer, field = "ZL")
# Use rasterize to transfer values from sf_object$a1 to the raster layer
rasterized_layer <- rasterize(sf_object, raster_layer, field = "zl")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
install.packages("rgdal")
install.packages("raster")
source("~/.active-rstudio-document")
library(raster)
library(rgdal)
shapefile_path <- 'E:/zyf_gy/zyf_gy_temp/temp4.shp'
s1 <- readOGR(dsn = shapefile_path, layer = basename(shapefile_path), stringsAsFactors = FALSE)
s1 <- st_read(dsn = shapefile_path, layer = basename(shapefile_path), stringsAsFactors = FALSE)
s1 <- st_read(shapefile_path)
r1 <- raster(s1, layer = "a1")
shapefile_path <- 'E:/zyf_gy/zyf_gy_temp/temp24.tif'
s1 <- raster(shapefile_path)
plot(s1)
library(raster)
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.05)
plot(r)
shape_r = rasterize(shape, r, 1)
plot(r)
plot(shape_r)
plot(shape,add=T)
plot(shape_r)
library(raster)
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.001)
shape_r = rasterize(shape, r, 'zl')
shape_r = rasterize(shape, r, 'ZL')
plot(shape,add=T)
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
library(raster)
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.01)
shape_r = rasterize(shape, r, 2)
plot(shape_r)
writeRaster(shape_r, "E:/zyf_gy/zyf_gy_temp/output.tif", format="Tiff")
writeRaster(shape_r, "E:/zyf_gy/zyf_gy_temp/output.tif", format="GTiff")
writeRaster(shape_r, "E:/zyf_gy/zyf_gy_temp/output.tif")
writeRaster(shape_r, "E:/zyf_gy/zyf_gy_temp/output.tif")
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.00001)
shape_r = rasterize(shape, r, 2)
plot(shape_r)
plot(shape,add=T)
shape_r = rasterize(shape, r, 'zl')
shape_r = rasterize(shape, r, 'ZL')
shape_r = rasterize(shape, r, zl)
source("~/.active-rstudio-document")
shape_r = rasterize(shape, r, "ID_2")
plot(shape_r)
plot(shape,add=T)
shape_r = rasterize(shape, r, "zl")
library(raster)
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.00001)
shape_r = rasterize(shape, r, "zl")
plot(shape,add=T)
plot(shape,add=T)
library(raster)
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
shape_r = rasterize(shape, r, zl)
"
shape_r = rasterize(shape, r, "zl")
shape_r = rasterize(shape, r, "zl")
shape = shapefile("E:/zyf_gy/zyf_gy_temp/temp4.shp")
r = raster(shape, res=0.00001)
shape_r = rasterize(shape, r, "zl")
source("~/.active-rstudio-document")
View(r)
View(r)
View(shape_r)
source("E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_basic_statistics_2023a/stepwise_regression1_up2023_1130.R")
intercept_only <- lm(mpg ~ 1, data=mtcars) #define intercept-only model
intercept_only <- lm(mpg ~ 1, data=mtcars) #define intercept-only model
all <- lm(mpg ~ ., data=mtcars) #define model with all predictors
#perform forward stepwise regression
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova #view results of forward stepwise regression
forward$coefficients #view final model
#perform backward stepwise regression
backward <- step(all, direction='backward', scope=formula(all), trace=0)
backward$anova #view results of backward stepwise regression
backward$coefficients
#perform both-direction stepwise regression
both <- step(intercept_only, direction='both', scope=formula(all), trace=0)
both$anova
both$coefficients
data(mtcars)
full_model <- lm(mpg ~ ., data = mtcars)
step_model <- step(full_model) # Perform stepwise regression
summary(step_model)  # Print the results
predicted_values <- predict(step_model, newdata = mtcars)  # Make predictions on the training data
residuals <- mtcars$mpg - predicted_values  # Calculate residuals
squared_residuals <- residuals^2  # Calculate squared residuals
mse <- mean(squared_residuals)  # Calculate mean squared error (MSE)
rmse <- sqrt(mse)   # Calculate root mean squared error (RMSE)
cat("Root Mean Squared Error (RMSE):", rmse, "\n") # Print RMSE
buffers <- c(200,400,600,800,1000) #to_be_set
length(buffers)
intercept_only <- lm(mpg ~ 1, data=mtcars) #define intercept-only model
all <- lm(mpg ~ ., data=mtcars) #define model with all predictors
#perform forward stepwise regression
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova #view results of forward stepwise regression
forward$coefficients #view final model
#perform backward stepwise regression
backward <- step(all, direction='backward', scope=formula(all), trace=0)
backward$anova #view results of backward stepwise regression
backward$coefficients
#perform both-direction stepwise regression
both <- step(intercept_only, direction='both', scope=formula(all), trace=0)
both$anova
both$coefficients
data(mtcars)
full_model <- lm(mpg ~ ., data = mtcars)
step_model <- step(full_model) # Perform stepwise regression
summary(step_model)  # Print the results
predicted_values <- predict(step_model, newdata = mtcars)  # Make predictions on the training data
residuals <- mtcars$mpg - predicted_values  # Calculate residuals
squared_residuals <- residuals^2  # Calculate squared residuals
mse <- mean(squared_residuals)  # Calculate mean squared error (MSE)
rmse <- sqrt(mse)   # Calculate root mean squared error (RMSE)
cat("Root Mean Squared Error (RMSE):", rmse, "\n") # Print RMSE
intercept_only <- lm(mpg ~ 1, data=mtcars) #define intercept-only model
all <- lm(mpg ~ ., data=mtcars) #define model with all predictors
#perform forward stepwise regression
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova #view results of forward stepwise regression
forward$coefficients #view final model
#perform backward stepwise regression
backward <- step(all, direction='backward', scope=formula(all), trace=0)
backward$anova #view results of backward stepwise regression
backward$coefficients
#perform both-direction stepwise regression
both <- step(intercept_only, direction='both', scope=formula(all), trace=0)
both$anova
both$coefficients
data(mtcars)
full_model <- lm(mpg ~ ., data = mtcars)
step_model <- step(full_model) # Perform stepwise regression
summary(step_model)  # Print the results
predicted_values <- predict(step_model, newdata = mtcars)  # Make predictions on the training data
residuals <- mtcars$mpg - predicted_values  # Calculate residuals
squared_residuals <- residuals^2  # Calculate squared residuals
mse <- mean(squared_residuals)  # Calculate mean squared error (MSE)
rmse <- sqrt(mse)   # Calculate root mean squared error (RMSE)
cat("Root Mean Squared Error (RMSE):", rmse, "\n") # Print RMSE
library(sf)
# Load the KML file
kml_file <- "E:/zyf_gn/zyf_gn_temp/自动追踪_20231203152410221.kmz"
sf_object <- st_read(kml_file)
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
#refer to: https://www.bilibili.com/video/BV1tV41117cU/?spm_id_from=333.337.search-card.all.click&vd_source=5ead28bbf00e6798e790cb439bf6f631
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
View(trainset)
#set model
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
#as.data.frame()
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
#na.roughfix() #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
skim(boston) #再次鸟瞰数据
hist(boston$medv, breaks = 50) #对因变量做直方图
#分类数据
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
#构建公式
form_reg <- as.formula(paste0('medv ~', paste(colnames(traindata)[1:13], collapse = ' + '))) #as.formula(): 将字符串转换成公式
form_reg
#modelling
set.seed(2)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
View(fit_rf_reg)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
# 变量重要性图示，默认最多显示30个变量
importance(fit_rf_reg) #变量重要性
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
View(fit_rf_reg)
View(c_rf_f2_res)
library(randomForest)
library(skimr)
library(DataExplorer)
library(tidyverse)
library(caret)
library(pROC)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
#as.data.frame()
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
#na.roughfix() #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
skim(boston) #再次鸟瞰数据
hist(boston$medv, breaks = 50) #对因变量做直方图
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
#构建公式
form_reg <- as.formula(paste0('medv ~', paste(colnames(traindata)[1:13], collapse = ' + '))) #as.formula(): 将字符串转换成公式
form_reg
#modelling
set.seed(2)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
plot(fit_rf_reg, main = "树的棵树与袋外MSE") # ntree参数与error之间的关系图示
# 变量重要性图示，默认最多显示30个变量
importance(fit_rf_reg) #变量重要性
fit_rf_reg
fit_rf_reg$call
fit_rf_reg$call$importance
fit_rf_reg$call$``
fit_rf_reg$rsq
fit_rf_reg
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainlinmod <- lm(trainpred ~ traindata$medv)
testpred <- predict(fit_rf_reg, newdata = testdata)  #测试集预测结果
defaultSummary(data.frame(obs = testdata$medv, pred = testpred))  #测试集预测误差指标
plot(x = testdata$medv, y = testpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林——实际值与预测值比较", sub = "测试集") # 图示测试集预测结果
a1 <- defaultSummary(data.frame(obs = testdata$medv, pred = testpred))
a1
a1[2]
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_cor_v2_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_BRT_1_v3_up2023_1123.R")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_BRT_1_v3_up2023_1123.R")
source("~/.active-rstudio-document")
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
library(MASS)
library(gbm)
data(Boston)
str(Boston)
set.seed(123)
split <- sample(nrow(Boston), nrow(Boston) * 0.7)
train <- Boston[split, ]
test <- Boston[-split, ]
set.seed(123)
fit_1 <- gbm(medv~., data = train, verbose = TRUE, shrinkage = 0.01, interaction.depth = 3,
n.minobsinnode = 5, n.trees = 5000, cv.folds = 10)
print(fit_1)
summary(fit_1)
perf_gbm1 = gbm.perf(fit_1, method = "cv")
perf_gbm1
predict1 <- predict(fit_1, test, perf_gbm1) #生成预测数据
plot(test$medv, predict1, main = 'Test dataset', xlab = 'original data', ylab = 'Predicted data')
#将原数据值和预测值作散点图
abline(1, 1)
plot.gbm(fit_1, i.var = 1) #生成第1个变量crim的偏依赖图
View(test)
View(fit_1)
source("E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/BRT_1_up2023_1026.R")
summary(fit_1)
print(fit_1)
View(test)
View(Boston)
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
#as.data.frame()
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
#na.roughfix() #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
boston
boston
boston
boston <- read.csv('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a/randon_forest_2/data-Boston4Reg.csv')
#as.data.frame()
skim(boston) #鸟瞰数据
plot_missing(boston) #数据缺失状况
#set_missing() #基于指定值填充
#na.roughfix() #基于代表值（如中位数、众数）填充
boston$chas <- factor(boston$chas) #转换为类型变量
skim(boston) #再次鸟瞰数据
hist(boston$medv, breaks = 50) #对因变量做直方图
#分类数据
set.seed(1)
trains <- createDataPartition(y = boston$medv, p = 0.75, list = F); #基于因变量拆分
traindata <- boston[trains, ]
testdata <- boston[-trains, ]
hist(traindata$medv, breaks = 50)
hist(testdata$medv, breaks = 50)
colnames(boston) #get the column names of boston
#构建公式
form_reg <- as.formula(paste0('medv ~', paste(colnames(traindata)[1:13], collapse = ' + '))) #as.formula(): 将字符串转换成公式
form_reg
#modelling
set.seed(2)
fit_rf_reg <- randomForest(form_reg, data = traindata, ntree = 500, mtry = 6, importance = T)
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
plot(fit_rf_reg, main = "树的棵树与袋外MSE") # ntree参数与error之间的关系图示
# 变量重要性图示，默认最多显示30个变量
importance(fit_rf_reg) #变量重要性
varImpPlot(fit_rf_reg, main = "随机森林变量重要性")
# 偏依赖图: 因变量随特定自变量变化的变化
partialPlot(x = fit_rf_reg, pred.data = traindata, x.var = crim)
#确保pred.data数据为data.frame, x.var: 某个自变量名称
plot(medv ~ crim, data = traindata) #对应散点图
trainpred <- predict(object = fit_rf_reg, newdata = traindata) # 训练集预测结果
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
plot(x = traindata$medv, y = trainpred, xlab = "Actual", ylab = "Prediction",
main = "随机森林—实际值与预测值比较", sub = "训练集") # 图示训练集预测结果
trainlinmod <- lm(trainpred ~ traindata$medv)
defaultSummary(data = data.frame(obs = traindata$medv, pred = trainpred)) # 训练集预测误差指标
testpred <- predict(fit_rf_reg, newdata = testdata)  #测试集预测结果
defaultSummary(data.frame(obs = testdata$medv, pred = testpred))  #测试集预测误差指标
predresult <- data.frame(obs = c(traindata$medv, testdata$medv), pred = c(trainpred, testpred),
group = c(rep('Train', length(trainpred)), rep('Test', length(testpred))))
ggplot(predresult, aes(x = obs, y = pred, fill = group, colour = group)) +
geom_point(shape = 21, size = 3) +
geom_smooth(method = 'lm', se = F, size = 1.2) +
geom_abline(intercept = 0, slope = 1, size = 1.2) +
labs(fill = NULL, colour = NULL) +
theme(legend.position = 'bottom')
#ntree:决策树棵树，比较大就可以. mtry: 每个节点可供选择的变量数目，2到10之间就可以
fit_rf_reg #模型概要
source("E:/zyf_gy/zyf_gy_2304_work/ppa_2301_k2/ppa_2301_ML_RF_1_v3_up2023_1123.R")
View(c_rf_f2_res)
c_rf_f2_res
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
c_rf_f2_res
source("~/.active-rstudio-document")
c_rf_f2_res
source("~/.active-rstudio-document")
c_rf_f2_res
source("~/.active-rstudio-document")
c_rf_f2_res
library(pacman)
p_load(randomForest, caret, pROC)
setwd('E:/zyf_gy/zyf_gy_2304_work/tool_codes_k1/R_lesson_machine_learning_2023a')
#input data
data('iris')
summary(iris)
#seperate data into trainset and testset
trainlist <- createDataPartition(iris$Species, p = 0.8, list = FALSE) #list: 逻辑值。true时，返回结果为列表形式，否则，为floor(p * length(y))行 times列的矩阵
trainset <- iris[trainlist,]
testset <- iris[-trainlist,]
#set model
set.seed(1)
rf.train <- randomForest(as.factor(Species) ~., data = trainset, importance = TRUE, na.action = na.pass)
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
plot(rf.train, main = 'randomforest origin');
#predict
set.seed(2)
rf.test <- predict(rf.train, newdata = testset, type = 'class') #该分析预测种类，因此type为class
rf.test
#结果统计
rf.cf <- confusionMatrix(as.factor(rf.test), as.factor(testset$Species))
#as.factor(rf.test)：预测结果；as.factor(testset$Species)：原结果
rf.cf
#ROC曲线和AUC值
rf.test2 <- predict(rf.train, newdata = testset, type = 'prob');
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf #值越接近1越好
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
#importance:显示变量的重要性排序; na.action = na.pass: 略过缺失值
rf.train
#input data
data('iris')
summary(iris)
